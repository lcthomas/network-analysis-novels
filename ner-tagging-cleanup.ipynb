{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up NER tagging results\n",
    "- This is a notebook I used to take a list of named entities from Stanford's NER tagger (people, in this case), and clean that list up to produce a list of all of the PERSON entitites in a text.\n",
    "- Most of this comes from this tutorial: https://erickpeirson.github.io/python/2015/05/01/named-entity-recognition.html. Thank you, Erick Peirson!\n",
    "- Need to change kernel of notebook to Python 2; here's how to do that: https://ipython.readthedocs.io/en/latest/install/kernel_install.html\n",
    "- This notebook assumes you have already produced a tagged list of entitites (people, locations, and objects) using Stanford's NER tagger via the command line. You can learn more about that here: https://erickpeirson.github.io/python/2015/05/01/named-entity-recognition.html\n",
    "    - There is an NER Python package, but I was never able to get it working correctly. Here's some info on that: https://www.npmjs.com/package/ner-server\n",
    "    - There is also the StanfordNERTagger available via the nltk package, but I also wasn't able to get that working correctly.\n",
    "- This tutorial also assumes that you have a relatively clean tagged document, with \",'/-...-- removed from the text (usually best to remove these prior to tagging, I have found). The code below is fairly brittle and won't work correctly if the tagged text doesn't break into entity:value pairs evenly (i.e., if there are 2 entities and 1 value in a tuple: ('-','-','O'). Punctuation characters often cause this to happen.\n",
    "- The usual cavaets about academic programming apply. I am an especially inexperienced Python programmer, so there are most definitely better ways to do this. I just don't know how to do them yet. But this worked for my purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file (tagged text).\n",
    "infile = open(\"littlelife_tagged.txt\")\n",
    "tagged_text = infile.read()\n",
    "infile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin parsing tagged text file.\n",
    "# Pull tags apart to create a list of tuples.\n",
    "tagged_tokens = [ tuple(ttok.split('/')) for ttok in tagged_text.split() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few items in list you just created.\n",
    "print(tagged_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the for-loop below will work.\n",
    "# If this creates a dictionary without a problem, you're ok to move on to the next step.\n",
    "# If there is an error, see the two commented out cells below.\n",
    "tagged_tokens_dict = dict(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these cells if function below isn't working (uncomment the code).\n",
    "# Take the number the dictionary error returns and use it to discover tuples with more than 2 elements in them.\n",
    "# This cell confirms that this is a tuple with more than 2 elements and tells you what they are.\n",
    "# tagged_tokens[305472]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one deletes them from the list.\n",
    "# tagged_tokens.pop(305472)\n",
    "\n",
    "# After deleting, go back up to the tagged_tokens_dict code above and repeat until there are no more errors.\n",
    "# If you've cleaned your text prior to tagging, hopefully you won't have too many of these errors.\n",
    "# If there are a lot, this will be very tedious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to figure out which tokens belong to each other as part of the same named entity.\n",
    "# Stanford's NER will split first and last names, for example.\n",
    "# But since Stanfordâ€™s NER treats punctuation characters as separate tokens (e.g. ./O), \n",
    "# we can be reasonably sure that when a sequence of tokens with the same tag occur together, they probably belong to the same named entity.\n",
    "# Generate a list of entities from a single tagged text. \n",
    "entities = []         # Named entity instances will go here.\n",
    "current_entity = []   # Tokens that are part of the current entity will go here.\n",
    "\n",
    "last_tag = None       # We'll use this to check whether a token is part of the same entity as the previous.\n",
    "\n",
    "for i in xrange(len(tagged_tokens)):    # Evaluate each token, in order. This is why you need Python 2, FYI. There is definitely a way to convert this to Python 3, but it's beyond me right now.\n",
    "\t# Separate the token from its tag, so that we can evaluate them separately.\n",
    "    token, tag = tagged_tokens[i]       \n",
    "\n",
    "    if tag == 'O' or last_tag != tag:\t# We've reached the end of the current entity.\n",
    "    \t# If that entity had a real tag (not 'O' or None), then save it.\n",
    "        if last_tag != 'O' and last_tag != None:\n",
    "        \t# We save the list of tokens in this named entity, along with its tag, as a tuple.\n",
    "        \t#  string.join() converts the list of tokens into a string.\n",
    "            entities.append((' '.join(current_entity), last_tag))\n",
    "        current_entity = []\t# Reset for a new entity.\n",
    "    last_tag = tag\t\t\t# Keep track of the current entity tag; see lines 10 and 12.\n",
    "    current_entity.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because I like knowing what's happened.\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our entities and their classes (tags), we can go in many different directions. \n",
    "# We need to pull out all of the entities and group them by class. \n",
    "# In the code below, we iterate over the list of (entity,tag) tuples, and sort the entities into a \n",
    "# dictionary (entities_binned) based on their tags.\n",
    "\n",
    "entities_binned = {}\n",
    "for entity, tag in entities:\n",
    "    # When we encounter a tag for the first time, we need to create a spot for it in our dictionary.\n",
    "    if tag not in entities_binned:    \n",
    "        entities_binned[tag] = []\n",
    "\n",
    "    entities_binned[tag].append(entity) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the resulting dictionary\n",
    "entities_binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we want just the character names in this specific case.\n",
    "char_names = entities_binned[\"PERSON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to json because I couldn't get it to write to a text file for some reason.\n",
    "# This will produce a simple json file of all of the entities from the PERSON category.\n",
    "# I then saved this json file as a plain text file and did some minor reg-exing in my text editor to \n",
    "# delete { and ' and extra spaces, resulting in a list of all of the PERSON entities in a text, where each entity is \n",
    "# on its own line. You will still have duplicate entities at this stage, so you will need to delete them.\n",
    "# It's fairly easy to do this using a text editor like the old-school TextWrangler.\n",
    "# Again, there are definitely ways to get Python to do this work for you, but it's easier for me\n",
    "# at this stage to simply do them myself in a text editor.\n",
    "# You can then customize this list further to create a character list (consolidating names and nicknames, including alternate names, deleting mistakes/historical figures, etc.).\n",
    "import json\n",
    " \n",
    "json = json.dumps(char_names)\n",
    "f = open(\"littlelife_char_names.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
